### 数组和链表的区别（1）

- **存储**
  - 数组时连续的内存空间，链表不需要连续。
- **长度**
  - 数组的长度需要预先确定，若超出数组则会溢出。
  - 链表的长度是动态扩展的。
- **随机访问**
  - 数组可以随机访问，时间复杂度为O(1)。
  - 链表不支持随机访问，平均需要O(n)。
- **插入，删除**
  - 数组其实位置的插入和删除， 时间复杂度为O(n)。
    - 数组的插入优化：如果与顺序无关，可以交换该位置和最后一位的下一位置的数值，然后插入。
    - 数组的删除优化：不做删除而是做标记，当数组没有存储空间的时候做一次删除操作。
  - 链表的时间复杂度为O(1)。
- **内存空间**
  - 链表占用的内存空间更大，因为需要额外的指针。

### 栈

**数组模拟栈**

```c++
// tt表示栈顶
int stk[N], tt = 0;

// 向栈顶插入一个数
stk[ ++ tt] = x;

// 从栈顶弹出一个数
tt -- ;

// 栈顶的值
stk[tt];

// 判断栈是否为空
if (tt > 0)
{

}
```

**栈实现浏览器的前进和后退功能**

使用两个栈，X 和 Y，把首次浏览的页面依次压入栈 X，当点击后退按钮时，再依次从栈 X 中出栈，并将出栈的数据依次放入栈 Y。当我们点击前进按钮时，我们依次从栈 Y 中取出数据，放入栈 X 中。当栈 X 中没有数据时，那就说明没有页面可以继续后退浏览了。当栈 Y 中没有数据，那就说明没有页面可以点击前进按钮浏览了。当通过页面 b 又跳转到新的页面 d ，页面 c 就无法再通过前进、后退按钮重复查看了，所以需要清空栈 Y。

### 队列

**数组模拟队列**

```c++
//1.普通队列
// hh 表示队头，tt表示队尾
int q[N], hh = 0, tt = -1;

// 向队尾插入一个数
q[ ++ tt] = x;

// 从队头弹出一个数
hh ++ ;

// 队头的值
q[hh];

// 判断队列是否为空
if (hh <= tt)
{

}

//2.循环队列
// hh 表示队头，tt表示队尾的后一个位置
int q[N], hh = 0, tt = 0;

// 向队尾插入一个数
q[tt ++ ] = x;
if (tt == N) tt = 0;

// 从队头弹出一个数
hh ++ ;
if (hh == N) hh = 0;

// 队头的值
q[hh];

// 判断队列是否为空
if (hh != tt)
{

}
```

### 排序算法

![综合排序算法分析](/Users/wushengna/manual/img/img-post/综合排序算法分析.jpg)

#### 冒泡排序

冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。优化操作当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作。

```c++
class Solution {
public:
    vector<int> MySort(vector<int>& arr) {
        int n = arr.size();
        for(int i = 0; i < n; i++){
            bool isch = false;
            for(int j = 0; j < n - i - 1; j++){
                if(arr[j] > arr[j + 1]){
                    swap(arr[j],arr[j + 1]);
                    isch = true;
                }
            }
            if(!isch){
                break;
            }
        }
        return arr;
    }
};
```

#### 插入排序

首先将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。

```c++
class Solution {
public:
    vector<int> MySort(vector<int>& arr) {
        int n = arr.size();
        for(int i = 1; i < n; i++){
            int s = arr[i];
            int j = i - 1;
            for( ; j >= 0; j--){
                if(arr[j] > s){
                    arr[j + 1] = arr[j];
                }
                else{
                    break;
                }
            }
            arr[j + 1] = s;
        }
        return arr;
    }
};
```

#### 选择排序

选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。

```c++
class Solution {
public:
    vector<int> MySort(vector<int>& arr) {
        int n = arr.size();
        for(int i = 0; i < n - 1; i++){
            int minInx = i;
            for(int j = i + 1; j < n; j++){
                if(arr[j] < arr[minInx]){
                    minInx = j;
                }
            }
            swap(arr[minInx],arr[i]);
        }
        return arr;
    }
};
```

#### 归并排序

使用分治思想，先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。

```c++
#include <iostream>

using namespace std;

const int N = 1e5 + 10;
int q[N], tmp[N];
int n;

void merge_sort(int q[], int l, int r){
    if(l >= r){
        return;
    }
    int mid = l + r >> 1;
    
    merge_sort(q,l,mid);
    merge_sort(q,mid + 1,r);
    
    int k = 0, i = l, j = mid + 1;
    while(i <= mid && j <= r){
        if(q[i] <= q[j]){
            tmp[k++] = q[i++];
        }
        else{
            tmp[k++] = q[j++];
        }
    }
    while(i <= mid){
        tmp[k++] = q[i++];
    }
    while(j <= r){
        tmp[k++] = q[j++];
    }
    for(int i = l, j = 0; i <= r; i++, j++){
        q[i] = tmp[j];
    }
    return;
}

int main(){
    scanf("%d",&n);
    for(int i = 0; i < n; i++){
        scanf("%d",&q[i]);
    }
    merge_sort(q,0,n - 1);
    for(int i = 0; i < n; i++){
        printf("%d ",q[i]);
    }
    return 0;
}
```

#### 快速排序

使用分治思想，如果要排序数组中下标从 p 到 r 之间的一组数据，选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。

```c++
#include <iostream>

using namespace std;

const int N = 1e5 + 10;
int q[N];
int n;

void quick_sort(int q[], int l, int r){
    if(l >= r){
        return;
    }
    int x = q[l + r >> 1], i = l - 1, j = r + 1;
    while(i < j){
        do{
            i++;
        }while(q[i] < x);
        do{
            j--;
        }while(q[j] > x);
        if(i < j){
            swap(q[i],q[j]);
        }
    }
    quick_sort(q,l,j); //边界条件，选择q[l + r >> 1]，区间要用[l,j]和[j + 1,r]
    quick_sort(q,j + 1,r);
    return;
}

int main(){
    scanf("%d",&n);
    for(int i = 0; i < n; i++){
        scanf("%d",&q[i]);
    }
    quick_sort(q,0,n - 1);
    for(int i = 0; i < n; i++){
        printf("%d ",q[i]);
    }
    return 0;
}
```

#### 桶排序

桶排序核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。

如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度为 O(k * logk)。m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(n*log(n/m))。当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。

**桶排序比较适合用在外部排序中**。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。比如说我们有 10GB 的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百 MB，没办法一次性把 10GB 的数据都加载到内存中。

可以先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，订单金额最小是 1 元，最大是 10 万元。将所有订单根据金额划分到 100 个桶里，第一个桶存储金额在 1 元到 1000 元之内的订单，第二桶存储金额在 1001 元到 2000 元之内的订单，以此类推。每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名（00，01，02...99）。理想的情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100MB 的订单数据，就可以将这 100 个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。

不过订单按照金额在 1 元到 10 万元之间并不一定是均匀分布的 ，所以 10GB 订单数据是无法均匀地被划分到 100 个文件中的。有可能某个金额区间的数据特别多，划分之后对应的文件就会很大，没法一次性读入内存。针对这些划分之后还是比较大的文件，可以继续划分，比如，订单金额在 1 元到 1000 元之间的比较多，就将这个区间继续划分为 10 个小区间，1 元到 100 元，101 元到 200 元，201 元到 300 元....901 元到 1000 元。如果划分之后，101 元到 200 元之间的订单还是太多，无法一次性读入内存，那就继续再划分，直到所有的文件都能读入内存为止。

#### 计数排序

当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。

先统计每个数值出现的次数，计算小于等于该数的数值有多少，把每个数值放在个数的位置，将个数减1，可以得到排序数组。

计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。

#### 基数排序

基数排序对要排序的数据是有要求的，**需要可以分割出独立的“位”来比较**，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。

假设有 10 万个手机号码，希望将这 10 万个手机号码从小到大排序，先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过 11 次排序之后，手机号码就都有序了。注意，这里按照每位来排序的排序算法要是稳定的，否则这个实现思路就是不正确的。

有时候要排序的数据并不都是等长的，比如我们排序牛津字典中的 20 万个英文单词，对于这种不等长的数据，可以把所有的单词补齐到相同长度，位数不够的可以在后面补“0”，因为根据ASCII值，所有字母都大于“0”，所以补“0”不会影响到原有的大小顺序，这样就可以继续用基数排序了。

#### 优化方法

STL中的sort并非只是普通的快速排序，除了对普通的快速排序进行优化，它还结合了插入排序和堆排序。当数据量较大时采用快速排序，分段递归，一旦分段后的数据量小于N，为避免递归调用带来过大的额外负荷，便会改用插入排序，因为在小规模数据面前，O(n^2) 时间复杂度的算法并不一定比 O(nlogn) 的算法执行时间长，而如果递归层次过深，有出现最坏情况的倾向，还会改用堆排序。

### 二分查找

使用二分查找的情况是可以找到一个条件，正好将数组分为两段，假设目标值在闭区间[l,r]中， 每次将区间长度缩小一半，当l = r时，我们就找到了目标值，二分查找分为整数二分和浮点数二分，整数二分需要考虑边界条件，二分查找的时间复杂度是O(logn)。

#### 整数二分

**版本1**
当我们将区间[l,r]划分成[l,mid]和[mid + 1,r]时，其更新操作是`r = mid`或者`l = mid + 1;`，计算mid时不需要加1。

```c++
int bsearch_1(int l, int r)
{
    while (l < r)
    {
        int mid = l + r >> 1;
        if (check(mid)) r = mid;
        else l = mid + 1;
    }
    return l;
}
```

**版本2**
当我们将区间[l,r]划分成[l,mid - 1]和[mid,r]时，其更新操作是`r = mid - 1`或者`l = mid;`，此时为了防止死循环，计算mid时需要加1。

```c++
int bsearch_2(int l, int r)
{
    while (l < r)
    {
        int mid = l + r + 1 >> 1;
        if (check(mid)) l = mid;
        else r = mid - 1;
    }
    return l;
}
```

#### 浮点数二分

```c++
bool check(double x) {/* ... */} // 检查x是否满足某种性质

double bsearch_3(double l, double r)
{
    const double eps = 1e-6;   // eps 表示精度，取决于题目对精度的要求
    while (r - l > eps)
    {
        double mid = (l + r) / 2;
        if (check(mid)) r = mid;
        else l = mid;
    }
    return l;
}
```

#### 应用情况

快速定位IP对应的省份地址。

- 二分查找需要使用数组。
- 二分查找针对的是具有一个性质可以把数组分成两份的应用，只能用在插入，删除等操作不频繁的应用中。
- 数据量太小不适合二分查找，顺序遍历就可以，如果数据之间的比较操作很耗时，则最好使用二分查找减小比较次数。
- 数据量太大不适合二分查找，需要连续的数组空间。

### 跳表

链表加多级索引的结构，就是跳表，Redis 中的有序集合（Sorted Set）就是用跳表来实现的，跳表的时间复杂度是O(logn)，而空间复杂度是O(n)，动态的插入和删除操作（删除需要删除节点和索引）也是O(logn)。为了维护平衡性，在插入的时候可以在节点和索引都插入，使用随机函数决定插入哪级索引层。

#### 跳表和红黑树的对比

- 插入、删除、查找以及迭代输出有序序列这几个操作，红黑树也可以完成，时间复杂度和跳表是一样。
- 跳表的优点
  - 代码相对简单。
  - 按照区间查找数据这个操作，红黑树的效率没有跳表高。跳表可以在 O(logn) 时间复杂度定位区间的起点，然后在原始链表中顺序向后查询就可以了，这样非常高效。
  - 删除一段区间的数据，相对于红黑树容易。

### 哈希表

散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是O(1)的特性。我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。

#### 散列冲突的解决方法

**拉链法**

当插入的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是 O(1)。当查找、删除一个元素时，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除。这两个操作的时间复杂度跟链表的长度k成正比，也就是O(k)，理论上讲，k=n/m，其中n表示散列中数据的个数，m表示散列表中“槽”的个数。基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。

```c++
int h[N], e[N], ne[N], idx;

// 向哈希表中插入一个数
void insert(int x)
{
  int k = (x % N + N) % N; //确定余数是正数
  e[idx] = x;
  ne[idx] = h[k];
  h[k] = idx ++ ;
}

// 在哈希表中查询某个数是否存在
bool find(int x)
{
  int k = (x % N + N) % N;
  for (int i = h[k]; i != -1; i = ne[i])
    if (e[i] == x)
      return true;

    return false;
}
```

**开放寻址法**

使用线性探测，二次探测和随机探测。当数据量比较小、装载因子小的时候，适合采用开放寻址法。

线性探测：当碰撞发生时即一个键的散列值被另外一个键占用时，直接检查散列表中的下一个位置即将索引值加1。

二次探测：索引值位移量不为1，为1，-1，2，-2...的平方。

随机探测：索引值位移量采用随机函数计算得到。随机种子产生伪随机数，每次得到的随机序列相同。

插入：如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。

查找：使用散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。

删除：可以将删除的元素特殊标记为deleted。当线性探测查找的时候，遇到标记为deleted的空间，并不是停下来，而是继续往下探测。

```c++
int h[N];

// 如果x在哈希表中，返回x的下标；如果x不在哈希表中，返回x应该插入的位置
// 开放两倍数组空间，null是数值范围外的质数
int find(int x)
{
  int t = (x % N + N) % N;
  while (h[t] != null && h[t] != x)
  {
    t ++ ;
    if (t == N) t = 0;
  }
  return t;
}
```

**再哈希法**

使用哈希函数去散列一个输入的时候，输出是同一个位置就再次散列，直至不发生冲突位置，但每次冲突都要重新散列，计算时间增加，另外需要准备多个哈希函数。

**公共溢出区法**

建立一个公共溢出区域，把hash冲突的元素都放在该溢出区里。查找时，如果发现hash表中对应桶里存在其他元素，还需要在公共溢出区里再次进行查找。

#### 应用情况

Word 文档中单词拼写检查功能，可以用散列表来存储整个英文单词词典，当用户输入某个英文单词时，我们拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。

**字符串哈希**

```c++
//核心思想：将字符串看成P进制数，P的经验值是131或13331，取这两个值的冲突概率低
//小技巧：取模的数用2^64，这样直接用unsigned long long存储，溢出的结果就是取模的结果

typedef unsigned long long ULL;
ULL h[N], p[N]; // h[k]存储字符串前k个字母的哈希值, p[k]存储 P^k mod 2^64

// 初始化
p[0] = 1;
for (int i = 1; i <= n; i ++ )
{
    h[i] = h[i - 1] * P + str[i];
    p[i] = p[i - 1] * P;
}

// 计算子串 str[l ~ r] 的哈希值
ULL get(int l, int r)
{
    return h[r] - h[l - 1] * p[r - l + 1];
}
```

安全加密，最常用于加密的哈希算法是 MD5（MD5 消息摘要算法）和 SHA（安全散列算法）。

唯一标识，在海量的图库中，搜索一张图是否存在，给每一个图片取一个唯一标识，或者说信息摘要。比如从图片的二进制码串开头取 100 个字节，从中间取 100 个字节，从最后再取 100 个字节，然后将这 300 个字节放到一块，通过哈希算法（比如 MD5），得到一个哈希字符串，用它作为图片的唯一标识。通过这个唯一标识来判定图片是否在图库中。还可以把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中。当要查看某个图片是不是在图库中的时候，先通过哈希算法对这个图片取唯一标识，然后在散列表中查找是否存在这个唯一标识。如果不存在，那就说明这个图片不在图库中；如果存在，我们再通过散列表中存储的文件路径，获取到这个已经存在的图片，跟现在要插入的图片做全量的比对，看是否完全一样。如果一样，就说明已经存在；如果不一样，说明两张图片尽管唯一标识相同，但是并不是相同的图片。

数据校验，防止BT下载错误，BT 下载的原理是基于 P2P 协议的。我们从多个机器上并行下载一个 2GB 的电影，这个电影文件可能会被分割成很多文件块（比如可以分成 100 块，每块大约 20MB）。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了。通过哈希算法，对 100 个文件块分别取哈希值，并且保存在种子文件中。当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。

散列函数

用哈希算法防止数据库中的信息被脱库，可以对用户密码进行加密之后再存储，针对字典攻击（维护一个常用密码的字典表，把字典中的每个密码用哈希算法计算哈希值，然后拿哈希值跟脱库后的密文比对。如果相同，基本上就可以认为，这个加密之后的密码对应的明文就是字典中的这个密码），我们可以引入一个盐，跟用户的密码组合在一起，增加密码的复杂度。我们拿组合之后的字符串来做哈希算法加密，将它存储到数据库中，进一步增加破解的难度。

负载均衡，可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。 这样就可以把同一个 IP 过来的所有请求，都路由到同一个后端服务器上。

数据分片，统计“搜索关键词”出现的次数，可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。具体的思路是：为了提高处理的速度，用 n 台机器并行处理。从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟 n 取模，最终得到的值，就是应该被分配到的机器编号。这样哈希值相同的搜索关键词就被分配到了同一个机器上。也就是说，同一个搜索关键词会被分配到同一个机器上。每个机器会分别计算关键词出现的次数，最后合并起来就是最终的结果。快速判断图片是否在图库中，假设图库的图片很大，可以对数据进行分片，然后采用多机处理。准备 n 台机器，让每台机器只维护某一部分图片对应的散列表。每次从图库中读取一个图片，计算唯一标识，然后与机器个数 n 求余取模，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数 n 求余取模。假设得到的值是 k，那就去编号 k 的机器构建的散列表中查找。

分布式存储，可以借用数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。一致性哈希解决雪崩效应，假设有 k 个机器，数据的哈希值的范围是[0, MAX]。我们将整个范围划分成 m 个小区间（m 远大于 k），每个机器负责 m/k 个小区间。当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。

#### 哈希扩充优化

**哈希表碰撞攻击**

攻击者可能通过精心构造的数据，使得所有的数据经过散列函数之后，都散列到同一个槽里。如果我们使用的是基于链表的冲突解决方法，那这个时候，散列表就会退化为链表，查询的时间复杂度就从 O(1) 急剧退化为 O(n)。如果散列表中有 10 万个数据，退化后的散列表查询的效率就下降了 10 万倍。这样就有可能因为查询操作消耗大量 CPU 或者线程资源，导致系统无法响应其他请求，从而达到拒绝服务攻击（DoS）的目的。

**动态扩容**

针对散列表，当装载因子过大时，我们也可以进行动态扩容，重新申请一个更大的散列表，将数据搬移到这个新散列表中。每次扩容我们都申请一个原来散列表大小两倍最近的质数的空间。

为了解决一次性扩容耗时过多的情况，我们可以**将扩容操作穿插在插入操作的过程中，分批完成**。当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。

对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，插入一个数据的时间复杂度都是 O(1)。

### 二叉树

完全二叉树是叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大的树，如果某棵二叉树是一棵完全二叉树，那用数组存储是最节省内存的一种方式（中间不会出现浪费）。

**非递归二叉树前序，中序，后序遍历**

```c++
//前序遍历
class Solution {
public:
    vector<int> preorderTraversal(TreeNode* root) {
        vector<int> res;
        stack<TreeNode*> stk;
        while(root || stk.size()){
            while(root){
                res.push_back(root->val);
                stk.push(root);
                root = root->left;
            }
            root = stk.top()->right;
            stk.pop();
        }
        return res;
    }
};
//中序遍历
class Solution {
public:
    vector<int> inorderTraversal(TreeNode* root) {
        vector<int> res;
        stack<TreeNode*> stk;
        while(root || stk.size()){
            while(root){
                stk.push(root);
                root = root->left;
            }
            res.push_back(stk.top()->val);
            root = stk.top()->right;
            stk.pop();
        }
        return res;
    }
};
//后序遍历
class Solution {
public:
    vector<int> postorderTraversal(TreeNode* root) {
        vector<int> res;
        stack<TreeNode*> stk;
        while(root || stk.size()){
            while(root){
                res.push_back(root->val);
                stk.push(root);
                root = root->right;
            }
            root = stk.top()->left;
            stk.pop();
        }
        reverse(res.begin(),res.end());
        return res;
    }
};
```

**有散列表为什么还需要二叉树**

- 散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，只需要中序遍历，就可以在 O(n) 的时间复杂度内，输出有序的数据序列。

- 散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在 O(logn)。

- 尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小，所以实际的查找速度可能不一定比 O(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。

- 散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。

- 为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。

### 红黑树（2）

**平衡二叉树：**二叉树中任意一个节点的左右子树的高度相差不能大于 1。平衡二叉查找树可以解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题。

**红黑树：**红黑树是一种不严格的平衡二叉查找树，红黑树中的节点，一类被标记为黑色，一类被标记为红色，而且满足要求：

- 根节点是黑色的；
- 每个叶子节点都是黑色的空节点（NIL），即叶子节点不存储数据（在具体实现的时候，只需要共用一个黑色的、空的叶子节点就行了。）；
- 任何相邻的节点都不能同时为红色，即红色节点是被黑色节点隔开的；
- 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；

红黑树查找时间复杂度O(logn)，插入和删除时间复杂度O(logn)。

**平衡树和红黑树的区别**

- **AVL树是高度平衡的**，频繁的插入和删除，会引起频繁的调整以重新平衡，导致效率下降。
- **红黑树不是高度平衡的**，算是一种折中，插入最多两次旋转，删除最多三次旋转，调整时新插入的都是红色。

**为什么红黑树的插入、删除和查找如此高效**

- 插入、删除和查找操作与树的高度成正比，因此如果平衡二叉树不会频繁的调整以重新平衡，那它肯定是最快的，但它需要频繁调整以保证平衡。
- 红黑树保证最长路径不超过最短路径的二倍，这种情况下插入最多两次旋转，删除最多三次旋转，因此比平衡二叉树高效。

**红黑树为什么要保证每条路径上黑色结点数目一致**

- 为了保证红黑树保证最长路径不超过最短路径的二倍。
- 假设一个红黑树T，其到叶节点的最短路径肯定全部是黑色节点（共B个），最长路径肯定有相同个黑色节点，另外会多几个红色节点。所以最长的路径长度应该是2B个节点，其中B个红色，B个黑色。

**左旋和右旋**

![左旋和右旋](/Users/wushengna/manual/img/img-post/左旋和右旋.jpg)

**插入操作的平衡调整**

红黑树规定，插入的节点必须是**红色的**。而且，二叉查找树中新插入的节点都是放在叶子节点上。

- 如果插入节点的父节点是黑色的，那我们什么都不用做，它仍然满足红黑树的定义。

- 如果插入的节点是根节点，那我们直接改变它的颜色，把它变成黑色就可以了。

- 需要调整改变颜色的情况：

  - CASE 1：如果关注节点是 a，它的叔叔节点 d 是红色。

    - 将关注节点 a 的父节点 b、叔叔节点 d 的颜色都设置成黑色；
    - 将关注节点 a 的祖父节点 c 的颜色设置成红色；
    - 关注节点变成 a 的祖父节点 c；
    - 跳到 CASE 2 或者 CASE 3。

    ![红黑树插入1](/Users/wushengna/manual/img/img-post/红黑树插入1.jpg)

  - CASE 2：如果关注节点是 a，它的叔叔节点 d 是黑色，关注节点 a 是其父节点 b 的右子节点。

    - 关注节点变成节点 a 的父节点 b；
    - 围绕新的关注节点b 左旋；
    - 跳到 CASE 3。

    ![红黑树插入2](/Users/wushengna/manual/img/img-post/红黑树插入2.jpg)

  - CASE 3：如果关注节点是 a，它的叔叔节点 d 是黑色，关注节点 a 是其父节点 b 的左子节点。

    - 围绕关注节点 a 的祖父节点 c 右旋；
    - 将关注节点 a 的父节点 b、兄弟节点 c 的颜色互换。
    - 调整结束。

    ![红黑树插入3](/Users/wushengna/manual/img/img-post/红黑树插入3.jpg)

**删除操作的平衡调整**

- 针对删除节点初步调整

  - CASE 1：如果要删除的节点是 a，它只有一个子节点 b。

    - 删除节点 a，并且把节点 b 替换到节点 a 的位置，这一部分操作跟普通的二叉查找树的删除操作一样；
    - 节点 a 只能是黑色，节点 b 也只能是红色，其他情况均不符合红黑树的定义。这种情况下，我们把节点 b 改为黑色；
    - 调整结束，不需要进行二次调整。

    ![红黑树删除1](/Users/wushengna/manual/img/img-post/红黑树删除1.jpg)

  - CASE 2：如果要删除的节点 a 有两个非空子节点，并且它的后继节点就是节点 a 的右子节点 c。

    - 如果节点 a 的后继节点就是右子节点 c，那右子节点 c 肯定没有左子树。我们把节点 a 删除，并且将节点 c 替换到节点 a 的位置。这一部分操作跟普通的二叉查找树的删除操作无异；
    - 然后把节点 c 的颜色设置为跟节点 a 相同的颜色；
    - 如果节点 c 是黑色，为了不违反红黑树的最后一条定义，我们给节点 c 的右子节点 d 多加一个黑色，这个时候节点 d 就成了“红 - 黑”或者“黑 - 黑”；
    - 这个时候，关注节点变成了节点 d，第二步的调整操作就会针对关注节点来做。

    ![红黑树删除2](/Users/wushengna/manual/img/img-post/红黑树删除2.jpg)

  - CASE 3：如果要删除的是节点 a，它有两个非空子节点，并且节点 a 的后继节点不是右子节点。

    - 找到后继节点 d，并将它删除，删除后继节点 d 的过程参照 CASE 1；
    - 将节点 a 替换成后继节点 d；
    - 把节点 d 的颜色设置为跟节点 a 相同的颜色；
    - 如果节点 d 是黑色，为了不违反红黑树的最后一条定义，我们给节点 d 的右子节点 c 多加一个黑色，这个时候节点 c 就成了“红 - 黑”或者“黑 - 黑”；
    - 这个时候，关注节点变成了节点 c，第二步的调整操作就会针对关注节点来做。

    ![红黑树删除3](/Users/wushengna/manual/img/img-post/红黑树删除3.jpg)

- 针对关注节点进行二次调整

  - CASE 1：如果关注节点是 a，它的兄弟节点 c 是红色的。

    - 围绕关注节点 a 的父节点 b 左旋；
    - 关注节点 a 的父节点 b 和祖父节点 c 交换颜色；
    - 关注节点不变；
    - 继续从四种情况中选择适合的规则来调整。

    ![红黑树删除4](/Users/wushengna/manual/img/img-post/红黑树删除4.jpg)

  - CASE 2：如果关注节点是 a，它的兄弟节点 c 是黑色的，并且节点 c 的左右子节点 d、e 都是黑色的。

    - 将关注节点 a 的兄弟节点 c 的颜色变成红色；
    - 从关注节点 a 中去掉一个黑色，这个时候节点 a 就是单纯的红色或者黑色；
    - 给关注节点 a 的父节点 b 添加一个黑色，这个时候节点 b 就变成了“红 - 黑”或者“黑 - 黑”；
    - 关注节点从 a 变成其父节点 b；
    - 继续从四种情况中选择符合的规则来调整。

    ![红黑树删除5](/Users/wushengna/manual/img/img-post/红黑树删除5.jpg)

  - CASE 3：如果关注节点是 a，它的兄弟节点 c 是黑色，c 的左子节点 d 是红色，c 的右子节点 e 是黑色。

    - 围绕关注节点 a 的兄弟节点 c 右旋；
    - 节点 c 和节点 d 交换颜色；
    - 关注节点不变；
    - 跳转到 CASE 4，继续调整。

    ![红黑树删除6](/Users/wushengna/manual/img/img-post/红黑树删除6.jpg)

  - CASE 4：如果关注节点 a 的兄弟节点 c 是黑色的，并且 c 的右子节点是红色的。

    - 围绕关注节点 a 的父节点 b 左旋；
    - 将关注节点 a 的兄弟节点 c 的颜色，跟关注节点 a 的父节点 b 设置成相同的颜色；
    - 将关注节点 a 的父节点 b 的颜色设置为黑色；
    - 从关注节点 a 中去掉一个黑色，节点 a 就变成了单纯的红色或者黑色；
    - 将关注节点 a 的叔叔节点 e 设置为黑色；
    - 调整结束。

    ![红黑树删除7](/Users/wushengna/manual/img/img-post/红黑树删除7.jpg)

### 堆

堆是完全二叉树，根据结点和左右孩子的大小关系，分为最大堆和最小堆。

- **最大堆**
  - 每个结点的值都大于或等于其左右孩子结点的值。
- **最小堆**
  - 每个结点的值都小于或等于其左右孩子结点的值。

建堆的时间复杂度是O(n)，往堆中插入一个元素和删除堆顶元素的时间复杂度都是 O(logn)，堆排序的时间复杂度是O(nlogn)，是原地排序算法，并不稳定。

#### 手写堆的操作

堆支持的操作：

1. 插入一个数：`heap[++size] = x; up(size);`
2. 求集合当中的最小值：`heap[1]`
3. 删除最小值：`heap[1] = heap[size]; size--; down(1);`
4. 删除任意一个元素（手写增加）：`heap[k] = heap[size]; size--; down(k); up(k);`
5. 修改任意一个元素（手写增加）：`heap[k] = x; down(k); up(k);`

```c++
// h[N]存储堆中的值, h[1]是堆顶，x的左儿子是2x, 右儿子是2x + 1
// ph[k]存储第k个插入的点在堆中的位置
// hp[k]存储堆中下标是k的点是第几个插入的
int h[N], ph[N], hp[N], size;

// 交换两个点，及其映射关系
void heap_swap(int a, int b)
{
    swap(ph[hp[a]],ph[hp[b]]);
    swap(hp[a], hp[b]);
    swap(h[a], h[b]);
}

void down(int u)
{
    int t = u;
    if (u * 2 <= size && h[u * 2] < h[t]) t = u * 2;
    if (u * 2 + 1 <= size && h[u * 2 + 1] < h[t]) t = u * 2 + 1;
    if (u != t)
    {
        heap_swap(u, t);
        down(t);
    }
}

void up(int u)
{
    while (u / 2 && h[u] < h[u / 2])
    {
        heap_swap(u, u / 2);
        u >>= 1;
    }
}

// O(n)建堆
for (int i = n / 2; i; i -- ) down(i);
```

#### 为什么快速排序要比堆排序性能好

- 堆排序数据访问的方式没有快速排序友好，对于快速排序来说，数据是顺序访问的，而对于堆排序来说，数据是跳着访问的，这样对 CPU 缓存是不友好的。
- 对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。

#### 堆的应用

- 快速获取到Top10最热门的搜索关键词，因为用户搜索的关键词，有很多可能都是重复的，所以首先要统计每个搜索关键词出现的频率，可以通过散列表、平衡二叉查找树或者其他一些支持快速查找、插入的数据结构，来记录关键词及其出现的次数。然后建立一个大小为 10 的小顶堆，遍历散列表，依次取出每个搜索关键词及对应出现的次数，然后与堆顶的搜索关键词对比。如果出现次数比堆顶搜索关键词的次数多，那就删除堆顶的关键词，将这个出现次数更多的关键词加入到堆中。以此类推，当遍历完整个散列表中的搜索关键词之后，堆中的搜索关键词就是出现次数最多的 Top 10 搜索关键词了。当我们无法一次性将所有的搜索关键词加入到内存中的时候，可以根据哈希算法的特点，将 10 亿条搜索关键词先通过哈希算法分片到 10 个文件中，创建 10 个空文件 00，01，02，……，09，遍历这 10 亿个关键词，并且通过某个哈希算法对其求哈希值，然后哈希值同 10 取模，得到的结果就是这个搜索关键词应该被分到的文件编号。对这 10 亿个关键词分片之后，每个文件都只有 1 亿的关键词，去除掉重复的，可能就只有 1000 万个，每个关键词平均 50 个字节，所以总的大小就是 500MB。1GB 的内存完全可以放得下。针对每个包含 1 亿条搜索关键词的文件，利用散列表和堆，分别求出 Top 10，然后把这个 10 个 Top 10 放在一块，然后取这 100 个关键词中，出现次数最多的 10 个关键词，这就是这 10 亿数据中的 Top 10 最频繁的搜索关键词了。
- 合并有序小文件。
- 高性能定时器，按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部（也就是小顶堆的堆顶）存储的是最先执行的任务。它拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔 T。这个时间间隔 T 就是从当前时间开始，需要等待多久，才会有第一个任务需要被执行。这样定时器就可以设定在 T 秒之后，再来执行任务。从当前时间点到（T-1）秒这段时间里，定时器都不需要做任何事情。当 T 秒时间过去之后，定时器取优先级队列中队首的任务执行。然后再计算新的队首任务的执行时间点与当前时间点的差值，把这个值作为定时器执行下一个任务需要等待的时间。这样定时器既不用间隔 1 秒就轮询一次，也不用遍历整个任务列表。

### 图

#### 存储方式

**邻接矩阵**

```c++
g[a][b] 存储边a->b，空间复杂度是O(n^2)
```

**邻接表**

```c++
// 对于每个点k，开一个单链表，存储k所有可以走到的点。h[k]存储这个单链表的头结点
int h[N], e[N], ne[N], idx;

// 添加一条边a->b
void add(int a, int b)
{
    e[idx] = b, ne[idx] = h[a], h[a] = idx ++ ;
}

// 初始化
idx = 0;
memset(h, -1, sizeof h);
```

#### 应用情况

- 存储微博，微信等社交软件的好友关系，比如微博的好友关系采用邻接表来存储，邻接表中存储了用户的关注关系，逆邻接表中存储的是用户的被关注关系。对应到图上，邻接表中，每个顶点的链表中，存储的就是这个顶点指向的顶点，逆邻接表中，每个顶点的链表中，存储的是指向这个顶点的顶点。如果要查找某个用户关注了哪些用户，可以在邻接表中查找；如果要查找某个用户被哪些用户关注了，可以从逆邻接表中查找。因为需要按照用户名称的首字母排序，分页来获取用户的粉丝列表或者关注列表，将邻接表中的链表改为支持快速查找的跳表更合适，跳表中存储的数据本来就是有序的了，分页获取粉丝列表或关注列表，就非常高效。对于大规模的数据，我们可以通过哈希算法等数据分片方式，将邻接表存储在不同的机器上。我们在机器 1 上存储顶点 1，2，3 的邻接表，在机器 2 上，存储顶点 4，5 的邻接表。逆邻接表的处理方式也一样。当要查询顶点与顶点关系的时候，我们就利用同样的哈希算法，先定位顶点所在的机器，然后再在相应的机器上查找。同样也可以利用外部存储（比如硬盘），因为外部存储的存储空间要比内存会宽裕很多。
- 社交软件亲密度。

### 字符串匹配

- **BF算法：**拿模式串与主串中的所有子串匹配，看是否有匹配子串，时间复杂度O(n*m)。

- **RK算法：**使用哈希算法对主串中的 n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。当不存在哈希冲突，如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了，当存在哈希冲突，如果某个子串的哈希值与模式串相等，就再对比一下子串和模式串本身是否相等，对比数值的效率比对比字符串的高，时间复杂度O(n*m)。

- **BM算法：**在模式串中某个字符与主串不能匹配的时候，将模式串往后多滑动几位，以此来减少不必要的字符比较，提高匹配的效率。BM 算法构建的规则有两类，坏字符规则和好后缀规则。好后缀规则可以独立于坏字符规则使用。

- **KMP算法：**KMP是一种高效的字符串匹配算法，用来在主字符串中查找模式字符串的位置，next数组用来存模式串中每个前缀最长的能匹配前缀子串的结尾字符的下标，空间复杂度O(m)，时间复杂度O(m + n)。

  **手写KMP算法**

  ```c++
  // s[]是长文本，p[]是模式串，m是s的长度，n是p的长度
  #include <iostream>
  
  using namespace std;
  
  const int N = 10010, M = 100010;
  int n, m;
  char p[N], s[M];
  int ne[N];
  
  int main(){
    cin >> n >> p + 1 >> m >> s + 1;
    
    //求next的过程
    for(int i = 2, j = 0; i <= n; i++){
      while(j && p[i] != p[j + 1]){
        j = ne[j];
      }
      if(p[i] == p[j + 1]){
        j++;
      }
      ne[i] = j;
    }
    
    //KMP匹配过程
    for(int i = 1, j = 0; i <= m; i++){
      while(j && s[i] != p[j + 1]){
        j = ne[j];
      }
      if(s[i] == p[j + 1]){
        j++;
      }
      if(j == n){
        printf("%d ",i - n);
        j = ne[j];
      }
    }
    return 0;
  }
  ```

- **Tire树：**是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题，Trie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。构建 Trie 树的过程，需要扫描所有的字符串，时间复杂度是 O(n)（n 表示所有字符串的长度和），构建好 Trie 树后，在其中查找字符串的时间复杂度是 O(k)，k表示要查找的字符串的长度。

  **手写Trie树**

  ```c++
  int son[N][26], cnt[N], idx;
  // 0号点既是根节点，又是空节点
  // son[][]存储树中每个节点的子节点
  // cnt[]存储以每个节点结尾的单词数量
  
  // 插入一个字符串
  void insert(char *str)
  {
      int p = 0;
      for (int i = 0; str[i]; i ++ )
      {
          int u = str[i] - 'a';
          if (!son[p][u]) son[p][u] = ++ idx;
          p = son[p][u];
      }
      cnt[p] ++ ;
  }
  
  // 查询字符串出现的次数
  int query(char *str)
  {
      int p = 0;
      for (int i = 0; str[i]; i ++ )
      {
          int u = str[i] - 'a';
          if (!son[p][u]) return 0;
          p = son[p][u];
      }
      return cnt[p];
  }
  ```

- **AC自动机：**AC自动机实际上就是在Trie树之上，加了类似KMP的next数组，只不过此处的next数组是构建在树上。

**应用情况**

- 文本编辑器中的查找功能（BM算法）。
- 搜索引擎的搜索关键词提示功能（Trie树）。
- 输入法自动补全功能，IDE代码编辑器自动补全功能，浏览器网址输入的自动补全功能。
- 敏感词过滤功能（AC自动机）。

### 贪心，分治，回溯，动态规划

- 霍夫曼编码实现数据压缩（贪心算法）。
- 海量订单排序（分治算法）。
- 购物凑单问题（动态规划）。
- 搜索引擎中的拼写纠错功能（动态规划），拿这个单词跟词库中的单词一一进行比较，计算编辑距离，将编辑距离最小的单词，作为纠正之后的单词，提示给用户。

### 拓扑排序

拓扑序列：如果由一个图的每个点构成的序列满足对于图的每条边(x,y)，序列中x都在y的前面，则称该序列是拓扑序列，有向无环图一定存在拓扑序列，有环就没有拓扑序列。

```c++
//时间复杂度O(n+m),n表示点数，m表示边数
bool topsort()
{
    int hh = 0, tt = -1;

    // d[i] 存储点i的入度
    for (int i = 1; i <= n; i ++ )
        if (!d[i])
            q[ ++ tt] = i;

    while (hh <= tt)
    {
        int t = q[hh ++ ];

        for (int i = h[t]; i != -1; i = ne[i])
        {
            int j = e[i];
            if (-- d[j] == 0)
                q[ ++ tt] = j;
        }
    }

    // 如果所有点都入队了，说明存在拓扑序列；否则不存在拓扑序列。
    return tt == n - 1;
}
```

**应用情况**

- 确定源文件的依赖关系。
- 排课问题。

### 最短路算法

**朴素Dijkstra算法**

```c++
//时间复杂是O(n^2+m)，n表示点数，m表示边数
int g[N][N];  // 存储每条边
int dist[N];  // 存储1号点到每个点的最短距离
bool st[N];   // 存储每个点的最短路是否已经确定

// 求1号点到n号点的最短路，如果不存在则返回-1
int dijkstra()
{
    memset(dist, 0x3f, sizeof dist);
    dist[1] = 0;

    for (int i = 0; i < n - 1; i ++ )
    {
        int t = -1;     // 在还未确定最短路的点中，寻找距离最小的点
        for (int j = 1; j <= n; j ++ )
            if (!st[j] && (t == -1 || dist[t] > dist[j]))
                t = j;

        // 用t更新其他点的距离
        for (int j = 1; j <= n; j ++ )
            dist[j] = min(dist[j], dist[t] + g[t][j]);

        st[t] = true;
    }

    if (dist[n] == 0x3f3f3f3f) return -1;
    return dist[n];
}
```

**Floyd算法**

```c++
//时间复杂度是O(n^3)，n表示点数
//初始化：
    for (int i = 1; i <= n; i ++ )
        for (int j = 1; j <= n; j ++ )
            if (i == j) d[i][j] = 0;
            else d[i][j] = INF;

// 算法结束后，d[a][b]表示a到b的最短距离
void floyd()
{
    for (int k = 1; k <= n; k ++ )
        for (int i = 1; i <= n; i ++ )
            for (int j = 1; j <= n; j ++ )
                d[i][j] = min(d[i][j], d[i][k] + d[k][j]);
}
```

**应用情况**

- 地图软件找到最优出行路径。

### 位图

bitmap（位图），每一位存放某种状态，通常用来判断某个数据是否存在。32位机器上，整型int在内存中占32bit(4 byte = 4 * 8 bit)，可以用对应的32bit对应十进制的0-31个数。

#### map映射表

**基本思想**

假设需要排序或者查找的总数N=10000000，那么我们需要申请内存空间的大小为**int a[1 + N/32]**，其中a[0]在内存中占32为可以对应十进制数0-31，依次类推： bitmap表为： a[0]--------->0-31 a[1]--------->32-63 a[2]--------->64-95 a[3]--------->96-127 ..........

**位映射步骤**

- 求十进制0-N对应在数组a中的下标：
  - 十进制0-31，对应在a[0]中，先由十进制数n转换为与32的模（商）可转化为对应在数组a中的下标。比如n=24,那么 n/32=0，则24对应在数组a中的下标为0。又比如n=60,那么n/32=1，则60对应在数组a中的下标为1，同理可以计算0-N在数组a中的下标。
- 求0-N对应每个下标0-31中的数：
  - 十进制0-31就对应0-31，而32-63则对应也是0-31，即给定一个数n可以通过模32的余数求得对应0-31中的数。
- 利用移位0-31使得对应位为1。

#### 应用情况

- 排序，去重，查找。
- 网页爬虫中URL查重。
- 2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。
  - **“内存空间不足以容纳这2.5亿个整数”我们可以快速的联想到Bitmap**。
  - 使用两个位图。
    - 第一个Bitmap存储的是整数是否出现。
    - 如果再次出现，则在第二个Bitmap中设置。
  - 使用一个位图。
    - 遍历一次这2.5亿个数字，如果对应的状态位为00，则将其变为01；如果对应的状态位为01，则将其变为11；如果为11，对应的转态位保持不变。
    - 最后，我们将状态位为01的进行统计，就得到了不重复的数字个数，时间复杂度为O(n)。

#### 布隆过滤器

**基本思想**

- 当一个元素被加入集合中时,通过k个散列函数将这个元素映射成一个位数组中的k个点,并将这k个点全部置为1。
- Bloom Filter使用k个相互独立的哈希函数（Hash Function），它们分别将集合中的每个元素映射到{1,…,m}的范围中。对任意一个元素x，第i个哈希函数映射的位置hi(x)就会被置为1（1≤i≤k）。注：如果一个位置多次被置为1，那么只有第一次会起作用，后面几次将没有任何效果。
- 在判断y是否属于这个集合时，对y应用k次哈希函数，若所有hi(y)的位置都是1（1≤i≤k），就认为y是集合中的元素，否则就认为y不是集合中的元素。

**缺点**

有一定的误判率，当判断某个元素在集合中，有可能是其他的元素的位；若集合中不存在该元素，则一定不存在。

### 朴素贝叶斯算法

- 使用过滤垃圾短信。

### 向量空间

- 通过欧几里得距离实现音乐推荐系统。

### 多路查找树

多路查找树，每一个结点的孩子数可以多于两个，每一个节点处可以存储多个元素。

#### 2-3树

每一个结点都具有两个孩子（2结点）或三个孩子（3结点）一个2结点包含一个元素和两个孩子，一个3结点包含一小一大两个元素和三个孩子。

### B/B+树

B树是一种平衡的多路查找树，2-3树和2-3-4树都是B树的特例，结点最大的孩子数目称为B树的阶。

#### B/B+树的区别

以（key，value）二元组来存储信息。

##### B树

- B树中的每个结点中既包含key，也包含value值，每个结点占用一个盘块的磁盘空间，一个结点上有m个升序排序的关键字和m + 1个指向子树根结点的指针，指针存储的是子结点所在磁盘块的地址。
- **缺点**：每个节点中不仅包含数据的key值，还有data值。而每一个页的存储空间是有限的，如果data数据较大时将会导致每个节点（即一个页）能存储的key的数量很小，当存储的数据量很大时同样会导致B树的深度较大，增大查询时的磁盘IO次数，进而影响查询效率。

##### B+树

- 所有数据记录节点都是按照键值大小顺序存放在同一层的叶子节点上，而非叶子节点上只存储key值信息，这样可以大大加大每个节点存储的key值数量，降低B+树的高度。
- B+树的头指针有两个，一个指向根节点，另一个指向关键字最小的元素，因此B+树有两种遍历的方式。
  - 从根节点开始随机查询。
  - 从最小关键词顺序查询。
- **优点**
  - 由于B+树在内部节点上不包含数据信息，因此在内存页中能够存放更多的key。
  - B+树的叶子结点都是相连的，因此对整棵树的便利只需要一次线性遍历叶子结点即可，方便随机查找和范围查找。

##### 区别

- 非叶子节点只存储键值信息，该值是其子树中的最大或最小值。

- 所有叶子节点之间都有一个链指针。

- 数据记录都存放在叶子节点中，叶子结点按照关键字的大小自小而大顺序连接。

- n棵子树的结点中包含n个关键字。

- 非叶子结点中的元素会在子树根结点中再次列出。

#### B树和红黑树应用场景

- **B树**
  - B/B+树是为了磁盘或其它存储设备而设计的一种平衡多路查找树（相对于二叉树，B树每个内节点有多个分支），与红黑树相比，在相同的节点的情况下，一颗B/B+树的高度远远小于红黑树的高度。
- **红黑树**
  - IO多路复用epoll的实现采用红黑树组织管理sockfd，以支持快速的增删改查。

### A*搜索算法

使用顶点到起点的距离和顶点到终点的直线长度作为更新条件。

- 可以实现游戏中的寻路功能。

### 索引

- **散列表**增删改查操作的性能非常好，时间复杂度是 O(1)。一些键值数据库，比如 Redis就是使用散列表来构建索引的。这类索引一般都构建在内存中。
- **红黑树**作为一种常用的平衡二叉查找树，数据插入、删除、查找的时间复杂度是 O(logn)，也非常适合用来构建内存索引。Ext 文件系统中，对磁盘块的索引，用的就是红黑树。
- **B+树**比起红黑树来说，更加适合构建存储在磁盘中的索引。B+树是一个多叉树，所以对相同个数的数据构建索引，B+树的高度要低于红黑树。当借助索引查询数据的时候，读取 B+树索引，需要的磁盘 IO 次数会更少。所以，大部分关系型数据库的索引，比如 MySQL、Oracle，都是用 B+树来实现的。
- **跳表**也支持快速添加、删除、查找数据。而且通过灵活调整索引结点个数和数据个数之间的比例，可以很好地平衡索引对内存的消耗及其查询效率。Redis 中的有序集合，就是用跳表来构建的。
- **布隆过滤器**有一定的判错率，尽管对于判定存在的数据，有可能并不存在，但是对于判定不存在的数据，那肯定就不存在。而且布隆过滤器内存占用非常少，我们可以针对数据，构建一个布隆过滤器，并且存储在内存中。当要查询数据的时候，我们可以先通过布隆过滤器，判定是否存在。如果通过布隆过滤器判定数据不存在，那我们就没有必要读取磁盘中的索引了。对于数据不存在的情况，数据查询就更加快速了。
- **有序数组**也可以被作为索引。如果数据是静态的，也就是不会有插入、删除、更新操作，那我们可以把数据的关键词（查询用的）抽取出来，组织成有序数组，然后利用二分查找算法来快速查找数据。

### 并行算法

- **并行排序**
  - 对归并排序并行化处理：将 8GB 的数据划分成 16 个小的数据集合，每个集合包含 500MB 的数据。用 16 个线程，并行地对这 16 个 500MB 的数据集合进行排序。这 16 个小集合分别排序完成之后，再将这 16 个有序集合合并。
  - 对快速排序并行化处理：通过扫描一遍数据，找到数据所处的范围区间，把这个区间从小到大划分成 16 个小区间，将 8GB 的数据划分到对应的区间中。针对这 16 个小区间的数据，启动 16 个线程，并行地进行排序，等到 16 个线程都执行结束之后，得到的数据就是有序数据了。
- **并行查找**
  - 可以将数据随机分割成 k 份（比如 16 份），每份中的数据只有原来的 1/k，然后我们针对这 k 个小数据集合分别构建散列表，这样散列表的维护成本就变低了。当某个小散列表的装载因子过大的时候，可以单独对这个散列表进行扩容，而其他散列表不需要进行扩容。假设有 2GB 的数据，放到 16 个散列表中，每个散列表中的数据大约是 150MB。当某个散列表需要扩容的时候，只需要额外增加 150*0.5=75MB 的内存（假设还是扩容到原来的 1.5 倍）。
  - 当要查找某个数据的时候，只需要通过 16 个线程，并行地在这 16 个散列表中查找数据。
  - 当往散列表中添加数据的时候，可以选择将这个新数据放入装载因子最小的那个散列表中，这样也有助于减少散列冲突。
- **并行字符串匹配**
  - 可以把大的文本，分割成 k 个小文本。假设 k 是 16，就启动 16 个线程，并行地在这 16 个小文本中查找关键词，这样整个查找的性能就提高了 16 倍。
  - 假设关键词的长度是 m，在每个小文本的结尾和开始各取 m 个字符串。前一个小文本的末尾 m 个字符和后一个小文本的开头 m 个字符，组成一个长度是 2m 的字符串，再拿关键词，在这个长度为 2m 的字符串中再重新查找一遍，就可以补上关键词被分割的问题了。
- **并行搜索**
  - 假设两个队列分别是队列 A 和队列 B，多线程并行处理队列 A 中的顶点，并将扩展得到的顶点存储在队列 B 中，等队列 A 中的顶点都扩展完成之后，队列 A 被清空，再并行地扩展队列 B 中的顶点，并将扩展出来的顶点存储在队列 A。这样两个队列循环使用，就可以实现并行广度优先搜索算法。